{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/core/datamodule.py:175: LightningDeprecationWarning: DataModule property `dims` was deprecated in v1.5 and will be removed in v1.7.\n",
      "  rank_zero_deprecation(\"DataModule property `dims` was deprecated in v1.5 and will be removed in v1.7.\")\n",
      "/opt/conda/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/core/datamodule.py:152: LightningDeprecationWarning: DataModule property `test_transforms` was deprecated in v1.5 and will be removed in v1.7.\n",
      "  rank_zero_deprecation(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from ddmimv4 import DDMIMV4\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pl_bolts.datamodules import ImagenetDataModule\n",
    "from config.option import parse_args\n",
    "# args= parse_args()\n",
    "\n",
    "model=DDMIMV4.load_from_checkpoint('log/seed1/version_203/checkpoints/last.ckpt')  \n",
    "datamodule=ImagenetDataModule(\"/root/study/imagenet2012\",num_workers=2,batch_size=4,pin_memory=False)\n",
    "\n",
    "test_loader=datamodule.test_dataloader()\n",
    "for index,batch in enumerate(test_loader):\n",
    "    loss=model.test_step(batch,index) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mSystemExit\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpl_bolts\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdatamodules\u001b[39;00m \u001b[39mimport\u001b[39;00m ImagenetDataModule\n\u001b[1;32m      6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39moption\u001b[39;00m \u001b[39mimport\u001b[39;00m parse_args\n\u001b[0;32m----> 7\u001b[0m args\u001b[39m=\u001b[39m parse_args()\n\u001b[1;32m      9\u001b[0m model\u001b[39m=\u001b[39mDDMIMV4\u001b[39m.\u001b[39mload_from_checkpoint(\u001b[39m'\u001b[39m\u001b[39mlog/seed1/version_203/checkpoints/last.ckpt\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     10\u001b[0m model    \n",
      "File \u001b[0;32m~/study/DDMIM/option.py:36\u001b[0m, in \u001b[0;36mparse_args\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m parser\u001b[39m.\u001b[39madd_argument(\u001b[39m'\u001b[39m\u001b[39m--gpu\u001b[39m\u001b[39m'\u001b[39m, default\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, \u001b[39mtype\u001b[39m\u001b[39m=\u001b[39m\u001b[39mint\u001b[39m,\n\u001b[1;32m     32\u001b[0m                 help\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mGPU id to use.\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     33\u001b[0m parser\u001b[39m.\u001b[39madd_argument(\u001b[39m'\u001b[39m\u001b[39m--log_dir\u001b[39m\u001b[39m'\u001b[39m, metavar\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mDIR\u001b[39m\u001b[39m'\u001b[39m, nargs\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m?\u001b[39m\u001b[39m'\u001b[39m, default\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mDDMIM/log\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m     34\u001b[0m                 help\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mpath to log (default: DDMIM/log)\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> 36\u001b[0m \u001b[39mreturn\u001b[39;00m parser\u001b[39m.\u001b[39;49mparse_args()\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.8/argparse.py:1771\u001b[0m, in \u001b[0;36mArgumentParser.parse_args\u001b[0;34m(self, args, namespace)\u001b[0m\n\u001b[1;32m   1769\u001b[0m \u001b[39mif\u001b[39;00m argv:\n\u001b[1;32m   1770\u001b[0m     msg \u001b[39m=\u001b[39m _(\u001b[39m'\u001b[39m\u001b[39munrecognized arguments: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m-> 1771\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49merror(msg \u001b[39m%\u001b[39;49m \u001b[39m'\u001b[39;49m\u001b[39m \u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m.\u001b[39;49mjoin(argv))\n\u001b[1;32m   1772\u001b[0m \u001b[39mreturn\u001b[39;00m args\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.8/argparse.py:2521\u001b[0m, in \u001b[0;36mArgumentParser.error\u001b[0;34m(self, message)\u001b[0m\n\u001b[1;32m   2519\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprint_usage(_sys\u001b[39m.\u001b[39mstderr)\n\u001b[1;32m   2520\u001b[0m args \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mprog\u001b[39m\u001b[39m'\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprog, \u001b[39m'\u001b[39m\u001b[39mmessage\u001b[39m\u001b[39m'\u001b[39m: message}\n\u001b[0;32m-> 2521\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mexit(\u001b[39m2\u001b[39;49m, _(\u001b[39m'\u001b[39;49m\u001b[39m%(prog)s\u001b[39;49;00m\u001b[39m: error: \u001b[39;49m\u001b[39m%(message)s\u001b[39;49;00m\u001b[39m\\n\u001b[39;49;00m\u001b[39m'\u001b[39;49m) \u001b[39m%\u001b[39;49m args)\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.8/argparse.py:2508\u001b[0m, in \u001b[0;36mArgumentParser.exit\u001b[0;34m(self, status, message)\u001b[0m\n\u001b[1;32m   2506\u001b[0m \u001b[39mif\u001b[39;00m message:\n\u001b[1;32m   2507\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_print_message(message, _sys\u001b[39m.\u001b[39mstderr)\n\u001b[0;32m-> 2508\u001b[0m _sys\u001b[39m.\u001b[39;49mexit(status)\n",
      "\u001b[0;31mSystemExit\u001b[0m: 2"
     ]
    }
   ],
   "source": [
    "%tb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 1, 1, 1, 1, 1)\n",
      "(1, 1)\n"
     ]
    }
   ],
   "source": [
    "print((2,)+(1,)*(5))\n",
    "print((1,)+(1,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from timm.models.layers import DropPath\n",
    "a=torch.nn.Module()\n",
    "a.b=DropPath(0.5)\n",
    "a.c=torch.nn.Module()\n",
    "with torch.no_grad():\n",
    "    a.c.train(False)\n",
    "    # a.c.training=False\n",
    "    print(a.training)\n",
    "    print(a.b.training)\n",
    "    print(a.c.training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 1., 1., 0.],\n",
      "        [1., 1., 0., 1.],\n",
      "        [0., 0., 0., 1.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "torch.manual_seed(1)\n",
    "x=torch.zeros([3,4])\n",
    "\n",
    "random_tensor = x.new_empty(x.shape).bernoulli_(0.5)\n",
    "print(random_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m在当前单元格或上一个单元格中执行代码时 Kernel 崩溃。请查看单元格中的代码，以确定故障的可能原因。有关详细信息，请单击 <a href='https://aka.ms/vscodeJupyterKernelCrash'>此处</a>。有关更多详细信息，请查看 Jupyter <a href='command:jupyter.viewOutput'>log</a>。"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.cls_token 1\n",
      "model.pos_embedding 1\n",
      "model.token_to_mask 768\n",
      "model.to_patch_embedding.1.weight 768\n",
      "model.to_patch_embedding.1.bias 768\n",
      "model.patch_to_emb.weight 768\n",
      "model.patch_to_emb.bias 768\n",
      "model.to_pixels.weight 768\n",
      "model.to_pixels.bias 768\n",
      "model.stages.0.layers.0.0.norm.weight 768\n",
      "model.stages.0.layers.0.0.norm.bias 768\n",
      "model.stages.0.layers.0.0.fn.to_qkv.weight 2304\n",
      "model.stages.0.layers.0.0.fn.to_out.0.weight 768\n",
      "model.stages.0.layers.0.0.fn.to_out.0.bias 768\n",
      "model.stages.0.layers.0.1.norm.weight 768\n",
      "model.stages.0.layers.0.1.norm.bias 768\n",
      "model.stages.0.layers.0.1.fn.net.0.weight 2048\n",
      "model.stages.0.layers.0.1.fn.net.0.bias 2048\n",
      "model.stages.0.layers.0.1.fn.net.3.weight 768\n",
      "model.stages.0.layers.0.1.fn.net.3.bias 768\n",
      "model.stages.0.layers.1.0.norm.weight 768\n",
      "model.stages.0.layers.1.0.norm.bias 768\n",
      "model.stages.0.layers.1.0.fn.to_qkv.weight 2304\n",
      "model.stages.0.layers.1.0.fn.to_out.0.weight 768\n",
      "model.stages.0.layers.1.0.fn.to_out.0.bias 768\n",
      "model.stages.0.layers.1.1.norm.weight 768\n",
      "model.stages.0.layers.1.1.norm.bias 768\n",
      "model.stages.0.layers.1.1.fn.net.0.weight 2048\n",
      "model.stages.0.layers.1.1.fn.net.0.bias 2048\n",
      "model.stages.0.layers.1.1.fn.net.3.weight 768\n",
      "model.stages.0.layers.1.1.fn.net.3.bias 768\n",
      "model.stages.1.layers.0.0.norm.weight 768\n",
      "model.stages.1.layers.0.0.norm.bias 768\n",
      "model.stages.1.layers.0.0.fn.to_qkv.weight 2304\n",
      "model.stages.1.layers.0.0.fn.to_out.0.weight 768\n",
      "model.stages.1.layers.0.0.fn.to_out.0.bias 768\n",
      "model.stages.1.layers.0.1.norm.weight 768\n",
      "model.stages.1.layers.0.1.norm.bias 768\n",
      "model.stages.1.layers.0.1.fn.net.0.weight 2048\n",
      "model.stages.1.layers.0.1.fn.net.0.bias 2048\n",
      "model.stages.1.layers.0.1.fn.net.3.weight 768\n",
      "model.stages.1.layers.0.1.fn.net.3.bias 768\n",
      "model.stages.1.layers.1.0.norm.weight 768\n",
      "model.stages.1.layers.1.0.norm.bias 768\n",
      "model.stages.1.layers.1.0.fn.to_qkv.weight 2304\n",
      "model.stages.1.layers.1.0.fn.to_out.0.weight 768\n",
      "model.stages.1.layers.1.0.fn.to_out.0.bias 768\n",
      "model.stages.1.layers.1.1.norm.weight 768\n",
      "model.stages.1.layers.1.1.norm.bias 768\n",
      "model.stages.1.layers.1.1.fn.net.0.weight 2048\n",
      "model.stages.1.layers.1.1.fn.net.0.bias 2048\n",
      "model.stages.1.layers.1.1.fn.net.3.weight 768\n",
      "model.stages.1.layers.1.1.fn.net.3.bias 768\n",
      "model.stages.2.layers.0.0.norm.weight 768\n",
      "model.stages.2.layers.0.0.norm.bias 768\n",
      "model.stages.2.layers.0.0.fn.to_qkv.weight 2304\n",
      "model.stages.2.layers.0.0.fn.to_out.0.weight 768\n",
      "model.stages.2.layers.0.0.fn.to_out.0.bias 768\n",
      "model.stages.2.layers.0.1.norm.weight 768\n",
      "model.stages.2.layers.0.1.norm.bias 768\n",
      "model.stages.2.layers.0.1.fn.net.0.weight 2048\n",
      "model.stages.2.layers.0.1.fn.net.0.bias 2048\n",
      "model.stages.2.layers.0.1.fn.net.3.weight 768\n",
      "model.stages.2.layers.0.1.fn.net.3.bias 768\n",
      "model.stages.2.layers.1.0.norm.weight 768\n",
      "model.stages.2.layers.1.0.norm.bias 768\n",
      "model.stages.2.layers.1.0.fn.to_qkv.weight 2304\n",
      "model.stages.2.layers.1.0.fn.to_out.0.weight 768\n",
      "model.stages.2.layers.1.0.fn.to_out.0.bias 768\n",
      "model.stages.2.layers.1.1.norm.weight 768\n",
      "model.stages.2.layers.1.1.norm.bias 768\n",
      "model.stages.2.layers.1.1.fn.net.0.weight 2048\n",
      "model.stages.2.layers.1.1.fn.net.0.bias 2048\n",
      "model.stages.2.layers.1.1.fn.net.3.weight 768\n",
      "model.stages.2.layers.1.1.fn.net.3.bias 768\n",
      "model.stages.2.layers.2.0.norm.weight 768\n",
      "model.stages.2.layers.2.0.norm.bias 768\n",
      "model.stages.2.layers.2.0.fn.to_qkv.weight 2304\n",
      "model.stages.2.layers.2.0.fn.to_out.0.weight 768\n",
      "model.stages.2.layers.2.0.fn.to_out.0.bias 768\n",
      "model.stages.2.layers.2.1.norm.weight 768\n",
      "model.stages.2.layers.2.1.norm.bias 768\n",
      "model.stages.2.layers.2.1.fn.net.0.weight 2048\n",
      "model.stages.2.layers.2.1.fn.net.0.bias 2048\n",
      "model.stages.2.layers.2.1.fn.net.3.weight 768\n",
      "model.stages.2.layers.2.1.fn.net.3.bias 768\n",
      "model.stages.2.layers.3.0.norm.weight 768\n",
      "model.stages.2.layers.3.0.norm.bias 768\n",
      "model.stages.2.layers.3.0.fn.to_qkv.weight 2304\n",
      "model.stages.2.layers.3.0.fn.to_out.0.weight 768\n",
      "model.stages.2.layers.3.0.fn.to_out.0.bias 768\n",
      "model.stages.2.layers.3.1.norm.weight 768\n",
      "model.stages.2.layers.3.1.norm.bias 768\n",
      "model.stages.2.layers.3.1.fn.net.0.weight 2048\n",
      "model.stages.2.layers.3.1.fn.net.0.bias 2048\n",
      "model.stages.2.layers.3.1.fn.net.3.weight 768\n",
      "model.stages.2.layers.3.1.fn.net.3.bias 768\n",
      "model.stages.2.layers.4.0.norm.weight 768\n",
      "model.stages.2.layers.4.0.norm.bias 768\n",
      "model.stages.2.layers.4.0.fn.to_qkv.weight 2304\n",
      "model.stages.2.layers.4.0.fn.to_out.0.weight 768\n",
      "model.stages.2.layers.4.0.fn.to_out.0.bias 768\n",
      "model.stages.2.layers.4.1.norm.weight 768\n",
      "model.stages.2.layers.4.1.norm.bias 768\n",
      "model.stages.2.layers.4.1.fn.net.0.weight 2048\n",
      "model.stages.2.layers.4.1.fn.net.0.bias 2048\n",
      "model.stages.2.layers.4.1.fn.net.3.weight 768\n",
      "model.stages.2.layers.4.1.fn.net.3.bias 768\n",
      "model.stages.2.layers.5.0.norm.weight 768\n",
      "model.stages.2.layers.5.0.norm.bias 768\n",
      "model.stages.2.layers.5.0.fn.to_qkv.weight 2304\n",
      "model.stages.2.layers.5.0.fn.to_out.0.weight 768\n",
      "model.stages.2.layers.5.0.fn.to_out.0.bias 768\n",
      "model.stages.2.layers.5.1.norm.weight 768\n",
      "model.stages.2.layers.5.1.norm.bias 768\n",
      "model.stages.2.layers.5.1.fn.net.0.weight 2048\n",
      "model.stages.2.layers.5.1.fn.net.0.bias 2048\n",
      "model.stages.2.layers.5.1.fn.net.3.weight 768\n",
      "model.stages.2.layers.5.1.fn.net.3.bias 768\n",
      "model.stages.3.layers.0.0.norm.weight 768\n",
      "model.stages.3.layers.0.0.norm.bias 768\n",
      "model.stages.3.layers.0.0.fn.to_qkv.weight 2304\n",
      "model.stages.3.layers.0.0.fn.to_out.0.weight 768\n",
      "model.stages.3.layers.0.0.fn.to_out.0.bias 768\n",
      "model.stages.3.layers.0.1.norm.weight 768\n",
      "model.stages.3.layers.0.1.norm.bias 768\n",
      "model.stages.3.layers.0.1.fn.net.0.weight 2048\n",
      "model.stages.3.layers.0.1.fn.net.0.bias 2048\n",
      "model.stages.3.layers.0.1.fn.net.3.weight 768\n",
      "model.stages.3.layers.0.1.fn.net.3.bias 768\n",
      "model.stages.3.layers.1.0.norm.weight 768\n",
      "model.stages.3.layers.1.0.norm.bias 768\n",
      "model.stages.3.layers.1.0.fn.to_qkv.weight 2304\n",
      "model.stages.3.layers.1.0.fn.to_out.0.weight 768\n",
      "model.stages.3.layers.1.0.fn.to_out.0.bias 768\n",
      "model.stages.3.layers.1.1.norm.weight 768\n",
      "model.stages.3.layers.1.1.norm.bias 768\n",
      "model.stages.3.layers.1.1.fn.net.0.weight 2048\n",
      "model.stages.3.layers.1.1.fn.net.0.bias 2048\n",
      "model.stages.3.layers.1.1.fn.net.3.weight 768\n",
      "model.stages.3.layers.1.1.fn.net.3.bias 768\n",
      "model.layer_norm.weight 768\n",
      "model.layer_norm.bias 768\n",
      "mlp_head.0.weight 768\n",
      "mlp_head.0.bias 768\n",
      "mlp_head.1.weight 1000\n",
      "mlp_head.1.bias 1000\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import config.option\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "# model=pl.LightningModule.load_from_checkpoint(\"log/seed3407/version_13/checkpoints/last.ckpt\")\n",
    "# cktp=torch.load(\"log/seed3407/version_13/checkpoints/last.ckpt\")\n",
    "from pytorch_lightning.plugins.io import TorchCheckpointIO as tcio\n",
    "# 实例化自己的model\n",
    "# nn_model = A()\n",
    "ckpt_path = \"log/seed1/version_129/checkpoints/last.ckpt\"\n",
    "# trainer = pl.Trainer(resume_from_checkpoint=ckpt_path)\n",
    "# 实例化函数\n",
    "tc = tcio()\n",
    "ckpt_dict = tc.load_checkpoint(path=ckpt_path)\n",
    "\n",
    "\n",
    "for key,v in ckpt_dict['state_dict'].items():\n",
    "    print(key,len(v))\n",
    "# print(len(ckpt_dict['state_dict']['projection_head.2.weight']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================================================================================================\n",
      "Layer (type:depth-idx)                                  Output Shape              Param #\n",
      "=========================================================================================================\n",
      "DDMIMV2                                                 [1, 197, 768]             744,960\n",
      "├─Rearrange: 1-1                                        [1, 196, 768]             --\n",
      "├─Linear: 1-2                                           [1, 196, 768]             590,592\n",
      "├─ModuleList: 1-3                                       --                        --\n",
      "│    └─Transformer: 2-1                                 [1, 197, 768]             --\n",
      "│    │    └─ModuleList: 3-1                             --                        --\n",
      "│    │    │    └─ModuleList: 4-1                        --                        --\n",
      "│    │    │    │    └─PreNorm: 5-1                      [1, 197, 768]             --\n",
      "│    │    │    │    │    └─LayerNorm: 6-1               [1, 197, 768]             1,536\n",
      "│    │    │    │    │    └─Attention: 6-2               [1, 197, 768]             2,360,064\n",
      "│    │    │    │    └─PreNorm: 5-2                      [1, 197, 768]             --\n",
      "│    │    │    │    │    └─LayerNorm: 6-3               [1, 197, 768]             1,536\n",
      "│    │    │    │    │    └─FeedForward: 6-4             [1, 197, 768]             3,148,544\n",
      "│    │    │    └─ModuleList: 4-2                        --                        --\n",
      "│    │    │    │    └─PreNorm: 5-3                      [1, 197, 768]             --\n",
      "│    │    │    │    │    └─LayerNorm: 6-5               [1, 197, 768]             1,536\n",
      "│    │    │    │    │    └─Attention: 6-6               [1, 197, 768]             2,360,064\n",
      "│    │    │    │    └─PreNorm: 5-4                      [1, 197, 768]             --\n",
      "│    │    │    │    │    └─LayerNorm: 6-7               [1, 197, 768]             1,536\n",
      "│    │    │    │    │    └─FeedForward: 6-8             [1, 197, 768]             3,148,544\n",
      "│    └─Transformer: 2-2                                 [1, 197, 768]             --\n",
      "│    │    └─ModuleList: 3-2                             --                        --\n",
      "│    │    │    └─ModuleList: 4-3                        --                        --\n",
      "│    │    │    │    └─PreNorm: 5-5                      [1, 197, 768]             --\n",
      "│    │    │    │    │    └─LayerNorm: 6-9               [1, 197, 768]             1,536\n",
      "│    │    │    │    │    └─Attention: 6-10              [1, 197, 768]             2,360,064\n",
      "│    │    │    │    └─PreNorm: 5-6                      [1, 197, 768]             --\n",
      "│    │    │    │    │    └─LayerNorm: 6-11              [1, 197, 768]             1,536\n",
      "│    │    │    │    │    └─FeedForward: 6-12            [1, 197, 768]             3,148,544\n",
      "│    │    │    └─ModuleList: 4-4                        --                        --\n",
      "│    │    │    │    └─PreNorm: 5-7                      [1, 197, 768]             --\n",
      "│    │    │    │    │    └─LayerNorm: 6-13              [1, 197, 768]             1,536\n",
      "│    │    │    │    │    └─Attention: 6-14              [1, 197, 768]             2,360,064\n",
      "│    │    │    │    └─PreNorm: 5-8                      [1, 197, 768]             --\n",
      "│    │    │    │    │    └─LayerNorm: 6-15              [1, 197, 768]             1,536\n",
      "│    │    │    │    │    └─FeedForward: 6-16            [1, 197, 768]             3,148,544\n",
      "│    └─Transformer: 2-3                                 [1, 197, 768]             --\n",
      "│    │    └─ModuleList: 3-3                             --                        --\n",
      "│    │    │    └─ModuleList: 4-5                        --                        --\n",
      "│    │    │    │    └─PreNorm: 5-9                      [1, 197, 768]             --\n",
      "│    │    │    │    │    └─LayerNorm: 6-17              [1, 197, 768]             1,536\n",
      "│    │    │    │    │    └─Attention: 6-18              [1, 197, 768]             2,360,064\n",
      "│    │    │    │    └─PreNorm: 5-10                     [1, 197, 768]             --\n",
      "│    │    │    │    │    └─LayerNorm: 6-19              [1, 197, 768]             1,536\n",
      "│    │    │    │    │    └─FeedForward: 6-20            [1, 197, 768]             3,148,544\n",
      "│    │    │    └─ModuleList: 4-6                        --                        --\n",
      "│    │    │    │    └─PreNorm: 5-11                     [1, 197, 768]             --\n",
      "│    │    │    │    │    └─LayerNorm: 6-21              [1, 197, 768]             1,536\n",
      "│    │    │    │    │    └─Attention: 6-22              [1, 197, 768]             2,360,064\n",
      "│    │    │    │    └─PreNorm: 5-12                     [1, 197, 768]             --\n",
      "│    │    │    │    │    └─LayerNorm: 6-23              [1, 197, 768]             1,536\n",
      "│    │    │    │    │    └─FeedForward: 6-24            [1, 197, 768]             3,148,544\n",
      "│    │    │    └─ModuleList: 4-7                        --                        --\n",
      "│    │    │    │    └─PreNorm: 5-13                     [1, 197, 768]             --\n",
      "│    │    │    │    │    └─LayerNorm: 6-25              [1, 197, 768]             1,536\n",
      "│    │    │    │    │    └─Attention: 6-26              [1, 197, 768]             2,360,064\n",
      "│    │    │    │    └─PreNorm: 5-14                     [1, 197, 768]             --\n",
      "│    │    │    │    │    └─LayerNorm: 6-27              [1, 197, 768]             1,536\n",
      "│    │    │    │    │    └─FeedForward: 6-28            [1, 197, 768]             3,148,544\n",
      "│    │    │    └─ModuleList: 4-8                        --                        --\n",
      "│    │    │    │    └─PreNorm: 5-15                     [1, 197, 768]             --\n",
      "│    │    │    │    │    └─LayerNorm: 6-29              [1, 197, 768]             1,536\n",
      "│    │    │    │    │    └─Attention: 6-30              [1, 197, 768]             2,360,064\n",
      "│    │    │    │    └─PreNorm: 5-16                     [1, 197, 768]             --\n",
      "│    │    │    │    │    └─LayerNorm: 6-31              [1, 197, 768]             1,536\n",
      "│    │    │    │    │    └─FeedForward: 6-32            [1, 197, 768]             3,148,544\n",
      "│    │    │    └─ModuleList: 4-9                        --                        --\n",
      "│    │    │    │    └─PreNorm: 5-17                     [1, 197, 768]             --\n",
      "│    │    │    │    │    └─LayerNorm: 6-33              [1, 197, 768]             1,536\n",
      "│    │    │    │    │    └─Attention: 6-34              [1, 197, 768]             2,360,064\n",
      "│    │    │    │    └─PreNorm: 5-18                     [1, 197, 768]             --\n",
      "│    │    │    │    │    └─LayerNorm: 6-35              [1, 197, 768]             1,536\n",
      "│    │    │    │    │    └─FeedForward: 6-36            [1, 197, 768]             3,148,544\n",
      "│    │    │    └─ModuleList: 4-10                       --                        --\n",
      "│    │    │    │    └─PreNorm: 5-19                     [1, 197, 768]             --\n",
      "│    │    │    │    │    └─LayerNorm: 6-37              [1, 197, 768]             1,536\n",
      "│    │    │    │    │    └─Attention: 6-38              [1, 197, 768]             2,360,064\n",
      "│    │    │    │    └─PreNorm: 5-20                     [1, 197, 768]             --\n",
      "│    │    │    │    │    └─LayerNorm: 6-39              [1, 197, 768]             1,536\n",
      "│    │    │    │    │    └─FeedForward: 6-40            [1, 197, 768]             3,148,544\n",
      "│    └─Transformer: 2-4                                 [1, 197, 768]             --\n",
      "│    │    └─ModuleList: 3-4                             --                        --\n",
      "│    │    │    └─ModuleList: 4-11                       --                        --\n",
      "│    │    │    │    └─PreNorm: 5-21                     [1, 197, 768]             --\n",
      "│    │    │    │    │    └─LayerNorm: 6-41              [1, 197, 768]             1,536\n",
      "│    │    │    │    │    └─Attention: 6-42              [1, 197, 768]             2,360,064\n",
      "│    │    │    │    └─PreNorm: 5-22                     [1, 197, 768]             --\n",
      "│    │    │    │    │    └─LayerNorm: 6-43              [1, 197, 768]             1,536\n",
      "│    │    │    │    │    └─FeedForward: 6-44            [1, 197, 768]             3,148,544\n",
      "│    │    │    └─ModuleList: 4-12                       --                        --\n",
      "│    │    │    │    └─PreNorm: 5-23                     [1, 197, 768]             --\n",
      "│    │    │    │    │    └─LayerNorm: 6-45              [1, 197, 768]             1,536\n",
      "│    │    │    │    │    └─Attention: 6-46              [1, 197, 768]             2,360,064\n",
      "│    │    │    │    └─PreNorm: 5-24                     [1, 197, 768]             --\n",
      "│    │    │    │    │    └─LayerNorm: 6-47              [1, 197, 768]             1,536\n",
      "│    │    │    │    │    └─FeedForward: 6-48            [1, 197, 768]             3,148,544\n",
      "=========================================================================================================\n",
      "Total params: 67,475,712\n",
      "Trainable params: 67,475,712\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (M): 66.73\n",
      "=========================================================================================================\n",
      "Input size (MB): 0.60\n",
      "Forward/backward pass size (MB): 141.61\n",
      "Params size (MB): 266.92\n",
      "Estimated Total Size (MB): 409.13\n",
      "=========================================================================================================\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchinfo\n",
    "import config.option\n",
    "import pytorch_lightning as pl\n",
    "from models.ddmimv2 import DDMIMV2\n",
    "\n",
    "# model=pl.LightningModule.load_from_checkpoint(\"log/seed3407/version_13/checkpoints/last.ckpt\")\n",
    "# cktp=torch.load(\"log/seed3407/version_13/checkpoints/last.ckpt\")\n",
    "from pytorch_lightning.plugins.io import TorchCheckpointIO as tcio\n",
    "# 实例化自己的model\n",
    "# nn_model = A()\n",
    "ckpt_path = \"log/seed1/version_122/checkpoints/last.ckpt\"\n",
    "# trainer = pl.Trainer(resume_from_checkpoint=ckpt_path)\n",
    "# 实例化函数\n",
    "tc = tcio()\n",
    "ckpt_dict = tc.load_checkpoint(path=ckpt_path)\n",
    "\n",
    "model=DDMIMV2.load_from_checkpoint(ckpt_path,dim=768)\n",
    "print(torchinfo.summary(model,depth=6,input_size=[1,3,224,224],device=\"cpu\"))\n",
    "\n",
    "# for key,v in ckpt_dict['state_dict'].items():\n",
    "#     print(key,len(v))\n",
    "# print(len(ckpt_dict['state_dict']['projection_head.2.weight']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================================================================================================\n",
      "Layer (type:depth-idx)                                  Output Shape              Param #\n",
      "=========================================================================================================\n",
      "DDMIMV2                                                 [1, 197, 768]             744,960\n",
      "├─Rearrange: 1-1                                        [1, 196, 768]             --\n",
      "├─Linear: 1-2                                           [1, 196, 768]             590,592\n",
      "├─ModuleList: 1-3                                       --                        --\n",
      "│    └─Transformer: 2-1                                 [1, 197, 768]             --\n",
      "│    │    └─ModuleList: 3-1                             --                        --\n",
      "│    │    │    └─ModuleList: 4-1                        --                        --\n",
      "│    │    │    │    └─PreNorm: 5-1                      [1, 197, 768]             --\n",
      "│    │    │    │    │    └─LayerNorm: 6-1               [1, 197, 768]             1,536\n",
      "│    │    │    │    │    └─Attention: 6-2               [1, 197, 768]             2,360,064\n",
      "│    │    │    │    └─PreNorm: 5-2                      [1, 197, 768]             --\n",
      "│    │    │    │    │    └─LayerNorm: 6-3               [1, 197, 768]             1,536\n",
      "│    │    │    │    │    └─FeedForward: 6-4             [1, 197, 768]             4,759,320\n",
      "│    │    │    └─ModuleList: 4-2                        --                        --\n",
      "│    │    │    │    └─PreNorm: 5-3                      [1, 197, 768]             --\n",
      "│    │    │    │    │    └─LayerNorm: 6-5               [1, 197, 768]             1,536\n",
      "│    │    │    │    │    └─Attention: 6-6               [1, 197, 768]             2,360,064\n",
      "│    │    │    │    └─PreNorm: 5-4                      [1, 197, 768]             --\n",
      "│    │    │    │    │    └─LayerNorm: 6-7               [1, 197, 768]             1,536\n",
      "│    │    │    │    │    └─FeedForward: 6-8             [1, 197, 768]             4,759,320\n",
      "│    └─Transformer: 2-2                                 [1, 197, 768]             --\n",
      "│    │    └─ModuleList: 3-2                             --                        --\n",
      "│    │    │    └─ModuleList: 4-3                        --                        --\n",
      "│    │    │    │    └─PreNorm: 5-5                      [1, 197, 768]             --\n",
      "│    │    │    │    │    └─LayerNorm: 6-9               [1, 197, 768]             1,536\n",
      "│    │    │    │    │    └─Attention: 6-10              [1, 197, 768]             2,360,064\n",
      "│    │    │    │    └─PreNorm: 5-6                      [1, 197, 768]             --\n",
      "│    │    │    │    │    └─LayerNorm: 6-11              [1, 197, 768]             1,536\n",
      "│    │    │    │    │    └─FeedForward: 6-12            [1, 197, 768]             4,759,320\n",
      "│    │    │    └─ModuleList: 4-4                        --                        --\n",
      "│    │    │    │    └─PreNorm: 5-7                      [1, 197, 768]             --\n",
      "│    │    │    │    │    └─LayerNorm: 6-13              [1, 197, 768]             1,536\n",
      "│    │    │    │    │    └─Attention: 6-14              [1, 197, 768]             2,360,064\n",
      "│    │    │    │    └─PreNorm: 5-8                      [1, 197, 768]             --\n",
      "│    │    │    │    │    └─LayerNorm: 6-15              [1, 197, 768]             1,536\n",
      "│    │    │    │    │    └─FeedForward: 6-16            [1, 197, 768]             4,759,320\n",
      "│    └─Transformer: 2-3                                 [1, 197, 768]             --\n",
      "│    │    └─ModuleList: 3-3                             --                        --\n",
      "│    │    │    └─ModuleList: 4-5                        --                        --\n",
      "│    │    │    │    └─PreNorm: 5-9                      [1, 197, 768]             --\n",
      "│    │    │    │    │    └─LayerNorm: 6-17              [1, 197, 768]             1,536\n",
      "│    │    │    │    │    └─Attention: 6-18              [1, 197, 768]             2,360,064\n",
      "│    │    │    │    └─PreNorm: 5-10                     [1, 197, 768]             --\n",
      "│    │    │    │    │    └─LayerNorm: 6-19              [1, 197, 768]             1,536\n",
      "│    │    │    │    │    └─FeedForward: 6-20            [1, 197, 768]             4,759,320\n",
      "│    │    │    └─ModuleList: 4-6                        --                        --\n",
      "│    │    │    │    └─PreNorm: 5-11                     [1, 197, 768]             --\n",
      "│    │    │    │    │    └─LayerNorm: 6-21              [1, 197, 768]             1,536\n",
      "│    │    │    │    │    └─Attention: 6-22              [1, 197, 768]             2,360,064\n",
      "│    │    │    │    └─PreNorm: 5-12                     [1, 197, 768]             --\n",
      "│    │    │    │    │    └─LayerNorm: 6-23              [1, 197, 768]             1,536\n",
      "│    │    │    │    │    └─FeedForward: 6-24            [1, 197, 768]             4,759,320\n",
      "│    │    │    └─ModuleList: 4-7                        --                        --\n",
      "│    │    │    │    └─PreNorm: 5-13                     [1, 197, 768]             --\n",
      "│    │    │    │    │    └─LayerNorm: 6-25              [1, 197, 768]             1,536\n",
      "│    │    │    │    │    └─Attention: 6-26              [1, 197, 768]             2,360,064\n",
      "│    │    │    │    └─PreNorm: 5-14                     [1, 197, 768]             --\n",
      "│    │    │    │    │    └─LayerNorm: 6-27              [1, 197, 768]             1,536\n",
      "│    │    │    │    │    └─FeedForward: 6-28            [1, 197, 768]             4,759,320\n",
      "│    │    │    └─ModuleList: 4-8                        --                        --\n",
      "│    │    │    │    └─PreNorm: 5-15                     [1, 197, 768]             --\n",
      "│    │    │    │    │    └─LayerNorm: 6-29              [1, 197, 768]             1,536\n",
      "│    │    │    │    │    └─Attention: 6-30              [1, 197, 768]             2,360,064\n",
      "│    │    │    │    └─PreNorm: 5-16                     [1, 197, 768]             --\n",
      "│    │    │    │    │    └─LayerNorm: 6-31              [1, 197, 768]             1,536\n",
      "│    │    │    │    │    └─FeedForward: 6-32            [1, 197, 768]             4,759,320\n",
      "│    │    │    └─ModuleList: 4-9                        --                        --\n",
      "│    │    │    │    └─PreNorm: 5-17                     [1, 197, 768]             --\n",
      "│    │    │    │    │    └─LayerNorm: 6-33              [1, 197, 768]             1,536\n",
      "│    │    │    │    │    └─Attention: 6-34              [1, 197, 768]             2,360,064\n",
      "│    │    │    │    └─PreNorm: 5-18                     [1, 197, 768]             --\n",
      "│    │    │    │    │    └─LayerNorm: 6-35              [1, 197, 768]             1,536\n",
      "│    │    │    │    │    └─FeedForward: 6-36            [1, 197, 768]             4,759,320\n",
      "│    │    │    └─ModuleList: 4-10                       --                        --\n",
      "│    │    │    │    └─PreNorm: 5-19                     [1, 197, 768]             --\n",
      "│    │    │    │    │    └─LayerNorm: 6-37              [1, 197, 768]             1,536\n",
      "│    │    │    │    │    └─Attention: 6-38              [1, 197, 768]             2,360,064\n",
      "│    │    │    │    └─PreNorm: 5-20                     [1, 197, 768]             --\n",
      "│    │    │    │    │    └─LayerNorm: 6-39              [1, 197, 768]             1,536\n",
      "│    │    │    │    │    └─FeedForward: 6-40            [1, 197, 768]             4,759,320\n",
      "│    └─Transformer: 2-4                                 [1, 197, 768]             --\n",
      "│    │    └─ModuleList: 3-4                             --                        --\n",
      "│    │    │    └─ModuleList: 4-11                       --                        --\n",
      "│    │    │    │    └─PreNorm: 5-21                     [1, 197, 768]             --\n",
      "│    │    │    │    │    └─LayerNorm: 6-41              [1, 197, 768]             1,536\n",
      "│    │    │    │    │    └─Attention: 6-42              [1, 197, 768]             2,360,064\n",
      "│    │    │    │    └─PreNorm: 5-22                     [1, 197, 768]             --\n",
      "│    │    │    │    │    └─LayerNorm: 6-43              [1, 197, 768]             1,536\n",
      "│    │    │    │    │    └─FeedForward: 6-44            [1, 197, 768]             4,759,320\n",
      "│    │    │    └─ModuleList: 4-12                       --                        --\n",
      "│    │    │    │    └─PreNorm: 5-23                     [1, 197, 768]             --\n",
      "│    │    │    │    │    └─LayerNorm: 6-45              [1, 197, 768]             1,536\n",
      "│    │    │    │    │    └─Attention: 6-46              [1, 197, 768]             2,360,064\n",
      "│    │    │    │    └─PreNorm: 5-24                     [1, 197, 768]             --\n",
      "│    │    │    │    │    └─LayerNorm: 6-47              [1, 197, 768]             1,536\n",
      "│    │    │    │    │    └─FeedForward: 6-48            [1, 197, 768]             4,759,320\n",
      "=========================================================================================================\n",
      "Total params: 86,805,024\n",
      "Trainable params: 86,805,024\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (M): 86.06\n",
      "=========================================================================================================\n",
      "Input size (MB): 0.60\n",
      "Forward/backward pass size (MB): 161.43\n",
      "Params size (MB): 344.24\n",
      "Estimated Total Size (MB): 506.27\n",
      "=========================================================================================================\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchinfo\n",
    "import config.option\n",
    "import pytorch_lightning as pl\n",
    "from models.ddmimv2 import DDMIMV2\n",
    "\n",
    "# model=pl.LightningModule.load_from_checkpoint(\"log/seed3407/version_13/checkpoints/last.ckpt\")\n",
    "# cktp=torch.load(\"log/seed3407/version_13/checkpoints/last.ckpt\")\n",
    "from pytorch_lightning.plugins.io import TorchCheckpointIO as tcio\n",
    "# 实例化自己的model\n",
    "# nn_model = A()\n",
    "ckpt_path = \"log/seed1/version_132/checkpoints/last.ckpt\"\n",
    "# trainer = pl.Trainer(resume_from_checkpoint=ckpt_path)\n",
    "# 实例化函数\n",
    "tc = tcio()\n",
    "ckpt_dict = tc.load_checkpoint(path=ckpt_path)\n",
    "\n",
    "model=DDMIMV2.load_from_checkpoint(ckpt_path,dim=768)\n",
    "print(torchinfo.summary(model,depth=6,input_size=[1,3,224,224],device=\"cpu\"))\n",
    "\n",
    "# for key,v in ckpt_dict['state_dict'].items():\n",
    "#     print(key,len(v))\n",
    "# print(len(ckpt_dict['state_dict']['projection_head.2.weight']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
